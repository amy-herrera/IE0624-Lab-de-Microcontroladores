{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vCA6Q6v9p9h"
      },
      "source": [
        "# Detector de fuegos por medio del uso de IA en plataforma Arduino NANO\n",
        "Abajo se definirá primero los requisitos necesarios para poder instanciar la red neuronal de tensorflow, así como las instalaciones pertinentes que debe tener la máquina virtual dentro de este ambiente virtual. Las dependencias que requiere el sistema son las siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jcSXD9H-ADq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3759ea4a-9194-47d0-d026-e8347f68e284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLukH30m_PoW"
      },
      "source": [
        "Ahora se van a definir las primeras instancias del programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlaxpFHz-KeC",
        "outputId": "ca62c102-ad35-406c-8dbc-490ced54739f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versión de tensorflow = 2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Se imporan las bibliotecas necesarias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import fileinput\n",
        "\n",
        "#Esto será relevante más adelante para la exportación a tensorflow lite\n",
        "print(f\"Versión de tensorflow = {tf.__version__}\")\n",
        "\n",
        "#Primero se va a definir una semilla aleatoria que no cambie cada vez que se instancie el programa dentro del notebook\n",
        "semilla = 6942\n",
        "np.random.seed(semilla)\n",
        "tf.random.set_seed(semilla)\n",
        "\n",
        "CLASSES = []\n",
        "#Se van a buscar todos los archivos con extensión .csv en el directorio del collab para así poder tomar los datos de muestra\n",
        "for file in os.listdir(\"/content/\"):\n",
        "  if file.endswith(\".csv\"):\n",
        "    CLASSES.append(os.path.splitext(file)[0])\n",
        "CLASSES.sort()\n",
        "\n",
        "SAMPLES_WINDOW_LEN = 1\n",
        "NUM_CLASSES = len(CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "xrR0_FIKAYZe",
        "outputId": "aae595e9-fb11-4ccf-84bc-4a8cbc64f6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32,4mFire\u001b[0n class will be output \u001b[32m0\u001b[0m of the classifier\n",
            "6267 samples captured for training with inputs ['Red', 'Green', 'Blue']\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32,4mnoFire\u001b[0n class will be output \u001b[32m1\u001b[0m of the classifier\n",
            "6368 samples captured for training with inputs ['Red', 'Green', 'Blue']\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La preparacion y el analisis de datos se completa satisfactoriamente\n",
            "Se organizaron los datos arbitrariamente en los sets: 'training, testing y validation' con exito.\n"
          ]
        }
      ],
      "source": [
        "#Se crea un one-hot para representar los datos y y leer cada conjunto para saber cuantos sets hay\n",
        "\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "#Leer cada csv para obtener un input y un output\n",
        "for class_index in range(NUM_CLASSES):\n",
        "  objectClass = CLASSES[class_index]\n",
        "  df = pd.read_csv('/content/'+objectClass+'.csv')\n",
        "  columns = list(df)\n",
        "\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  num_recordings = int(df.shape[0]/SAMPLES_WINDOW_LEN)\n",
        "  print(f'\\u001b[32,4m{objectClass}\\u001b[0n class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier')\n",
        "  print(f'{num_recordings} samples captured for training with inputs {list(df)}\\n')\n",
        "\n",
        "# Se grafican los datos\n",
        "  plt.rcParams[\"figure.figsize\"] =(10,1)\n",
        "  pixels = np.array([df['Red'],df['Green'],df['Blue']],float)\n",
        "  pixels = np.transpose(pixels)\n",
        "  for i in range(num_recordings):\n",
        "    plt.axvline(x=i,linewidth=8,color=tuple(pixels[i]/np.max(pixels[i], axis=0)))\n",
        "  plt.show()\n",
        "\n",
        "  #Tensores\n",
        "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
        "\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    row = []\n",
        "    for c in columns:\n",
        "      row.append(df[c][i])\n",
        "    tensor += row\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "#Se realiza la conversion de las listas\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"La preparacion y el analisis de datos se completa satisfactoriamente\")\n",
        "\n",
        "#La lista input se vuelve arbitraria para que los datos se organicen para el entrenamiento, las pruebas y la validacion\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "#Se hace la distribucion de grupos para en los tres sets: training, testing y validation\n",
        "TRAIN_SPLIT = int(0.6*num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Se organizaron los datos arbitrariamente en los sets: 'training, testing y validation' con exito.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Lxe0Ewi2Vv",
        "outputId": "7c086a77-6ce9-40f7-e3d5-9991c4b60a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.2345 - val_loss: 0.2129\n",
            "Epoch 2/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.2031 - val_loss: 0.1908\n",
            "Epoch 3/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1951 - val_loss: 0.1884\n",
            "Epoch 4/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1941 - val_loss: 0.1882\n",
            "Epoch 5/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1934 - val_loss: 0.1866\n",
            "Epoch 6/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1926 - val_loss: 0.1877\n",
            "Epoch 7/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1920 - val_loss: 0.1864\n",
            "Epoch 8/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1918 - val_loss: 0.1850\n",
            "Epoch 9/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1914 - val_loss: 0.1873\n",
            "Epoch 10/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1918 - val_loss: 0.1840\n",
            "Epoch 11/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1914 - val_loss: 0.1869\n",
            "Epoch 12/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1907 - val_loss: 0.1866\n",
            "Epoch 13/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1913 - val_loss: 0.1835\n",
            "Epoch 14/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1839\n",
            "Epoch 15/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1908 - val_loss: 0.1858\n",
            "Epoch 16/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1908 - val_loss: 0.1842\n",
            "Epoch 17/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1907 - val_loss: 0.1840\n",
            "Epoch 18/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1879\n",
            "Epoch 19/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1904 - val_loss: 0.1847\n",
            "Epoch 20/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1834\n",
            "Epoch 21/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1906 - val_loss: 0.1838\n",
            "Epoch 22/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1908 - val_loss: 0.1831\n",
            "Epoch 23/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1905 - val_loss: 0.1830\n",
            "Epoch 24/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1903 - val_loss: 0.1847\n",
            "Epoch 25/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1908 - val_loss: 0.1831\n",
            "Epoch 26/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1901 - val_loss: 0.1847\n",
            "Epoch 27/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1904 - val_loss: 0.1841\n",
            "Epoch 28/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1906 - val_loss: 0.1841\n",
            "Epoch 29/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1909 - val_loss: 0.1829\n",
            "Epoch 30/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1903 - val_loss: 0.1835\n",
            "Epoch 31/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1903 - val_loss: 0.1826\n",
            "Epoch 32/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1898 - val_loss: 0.1830\n",
            "Epoch 33/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1908 - val_loss: 0.1848\n",
            "Epoch 34/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1905 - val_loss: 0.1840\n",
            "Epoch 35/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1907 - val_loss: 0.1827\n",
            "Epoch 36/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1905 - val_loss: 0.1826\n",
            "Epoch 37/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1828\n",
            "Epoch 38/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1902 - val_loss: 0.1881\n",
            "Epoch 39/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1907 - val_loss: 0.1840\n",
            "Epoch 40/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1902 - val_loss: 0.1836\n",
            "Epoch 41/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1825\n",
            "Epoch 42/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1828\n",
            "Epoch 43/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1907 - val_loss: 0.1831\n",
            "Epoch 44/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1904 - val_loss: 0.1845\n",
            "Epoch 45/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1902 - val_loss: 0.1853\n",
            "Epoch 46/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1858\n",
            "Epoch 47/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1906 - val_loss: 0.1841\n",
            "Epoch 48/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1904 - val_loss: 0.1829\n",
            "Epoch 49/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1900 - val_loss: 0.1871\n",
            "Epoch 50/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1899 - val_loss: 0.1833\n",
            "Epoch 51/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1898 - val_loss: 0.1845\n",
            "Epoch 52/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1899 - val_loss: 0.1838\n",
            "Epoch 53/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1895 - val_loss: 0.1826\n",
            "Epoch 54/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1897 - val_loss: 0.1819\n",
            "Epoch 55/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1895 - val_loss: 0.1830\n",
            "Epoch 56/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1892 - val_loss: 0.1847\n",
            "Epoch 57/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1895 - val_loss: 0.1821\n",
            "Epoch 58/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1889 - val_loss: 0.1841\n",
            "Epoch 59/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1893 - val_loss: 0.1851\n",
            "Epoch 60/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1885 - val_loss: 0.1833\n",
            "Epoch 61/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1888 - val_loss: 0.1860\n",
            "Epoch 62/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1884 - val_loss: 0.1829\n",
            "Epoch 63/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1888 - val_loss: 0.1823\n",
            "Epoch 64/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1886 - val_loss: 0.1817\n",
            "Epoch 65/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1891 - val_loss: 0.1832\n",
            "Epoch 66/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1888 - val_loss: 0.1821\n",
            "Epoch 67/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1881 - val_loss: 0.1818\n",
            "Epoch 68/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1885 - val_loss: 0.1828\n",
            "Epoch 69/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1890 - val_loss: 0.1825\n",
            "Epoch 70/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1888 - val_loss: 0.1832\n",
            "Epoch 71/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1883 - val_loss: 0.1834\n",
            "Epoch 72/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1880 - val_loss: 0.1801\n",
            "Epoch 73/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1879 - val_loss: 0.1796\n",
            "Epoch 74/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1873 - val_loss: 0.1810\n",
            "Epoch 75/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1871 - val_loss: 0.1798\n",
            "Epoch 76/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1878 - val_loss: 0.1826\n",
            "Epoch 77/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1874 - val_loss: 0.1808\n",
            "Epoch 78/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1869 - val_loss: 0.1825\n",
            "Epoch 79/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1873 - val_loss: 0.1794\n",
            "Epoch 80/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1875 - val_loss: 0.1800\n",
            "Epoch 81/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1871 - val_loss: 0.1786\n",
            "Epoch 82/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1872 - val_loss: 0.1794\n",
            "Epoch 83/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1864 - val_loss: 0.1800\n",
            "Epoch 84/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1867 - val_loss: 0.1808\n",
            "Epoch 85/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1869 - val_loss: 0.1786\n",
            "Epoch 86/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1869 - val_loss: 0.1813\n",
            "Epoch 87/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1862 - val_loss: 0.1826\n",
            "Epoch 88/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1864 - val_loss: 0.1792\n",
            "Epoch 89/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1872 - val_loss: 0.1790\n",
            "Epoch 90/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1858 - val_loss: 0.1779\n",
            "Epoch 91/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1863 - val_loss: 0.1787\n",
            "Epoch 92/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1865 - val_loss: 0.1786\n",
            "Epoch 93/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1864 - val_loss: 0.1783\n",
            "Epoch 94/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1859 - val_loss: 0.1778\n",
            "Epoch 95/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1857 - val_loss: 0.1772\n",
            "Epoch 96/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1854 - val_loss: 0.1772\n",
            "Epoch 97/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1854 - val_loss: 0.1785\n",
            "Epoch 98/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1850 - val_loss: 0.1773\n",
            "Epoch 99/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1845 - val_loss: 0.1769\n",
            "Epoch 100/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1846 - val_loss: 0.1763\n",
            "Epoch 101/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1845 - val_loss: 0.1774\n",
            "Epoch 102/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1844 - val_loss: 0.1779\n",
            "Epoch 103/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1841 - val_loss: 0.1764\n",
            "Epoch 104/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1841 - val_loss: 0.1771\n",
            "Epoch 105/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1838 - val_loss: 0.1768\n",
            "Epoch 106/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1833 - val_loss: 0.1774\n",
            "Epoch 107/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1843 - val_loss: 0.1789\n",
            "Epoch 108/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1840 - val_loss: 0.1750\n",
            "Epoch 109/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1843 - val_loss: 0.1753\n",
            "Epoch 110/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1834 - val_loss: 0.1750\n",
            "Epoch 111/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1829 - val_loss: 0.1751\n",
            "Epoch 112/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1840 - val_loss: 0.1753\n",
            "Epoch 113/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1821 - val_loss: 0.1763\n",
            "Epoch 114/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1824 - val_loss: 0.1760\n",
            "Epoch 115/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1827 - val_loss: 0.1745\n",
            "Epoch 116/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1830 - val_loss: 0.1792\n",
            "Epoch 117/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1834 - val_loss: 0.1756\n",
            "Epoch 118/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1824 - val_loss: 0.1758\n",
            "Epoch 119/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1828 - val_loss: 0.1760\n",
            "Epoch 120/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1825 - val_loss: 0.1766\n",
            "Epoch 121/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1825 - val_loss: 0.1773\n",
            "Epoch 122/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1819 - val_loss: 0.1745\n",
            "Epoch 123/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1826 - val_loss: 0.1748\n",
            "Epoch 124/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1820 - val_loss: 0.1739\n",
            "Epoch 125/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1826 - val_loss: 0.1916\n",
            "Epoch 126/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1829 - val_loss: 0.1742\n",
            "Epoch 127/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1824 - val_loss: 0.1751\n",
            "Epoch 128/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1824 - val_loss: 0.1758\n",
            "Epoch 129/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1824 - val_loss: 0.1783\n",
            "Epoch 130/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1825 - val_loss: 0.1737\n",
            "Epoch 131/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1816 - val_loss: 0.1739\n",
            "Epoch 132/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1824 - val_loss: 0.1770\n",
            "Epoch 133/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1821 - val_loss: 0.1746\n",
            "Epoch 134/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1819 - val_loss: 0.1767\n",
            "Epoch 135/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1822 - val_loss: 0.1754\n",
            "Epoch 136/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1820 - val_loss: 0.1765\n",
            "Epoch 137/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1819 - val_loss: 0.1764\n",
            "Epoch 138/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1817 - val_loss: 0.1762\n",
            "Epoch 139/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1817 - val_loss: 0.1770\n",
            "Epoch 140/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1818 - val_loss: 0.1741\n",
            "Epoch 141/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1821 - val_loss: 0.1744\n",
            "Epoch 142/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1821 - val_loss: 0.1740\n",
            "Epoch 143/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1819 - val_loss: 0.1738\n",
            "Epoch 144/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1821 - val_loss: 0.1732\n",
            "Epoch 145/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1814 - val_loss: 0.1734\n",
            "Epoch 146/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1822 - val_loss: 0.1743\n",
            "Epoch 147/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1813 - val_loss: 0.1765\n",
            "Epoch 148/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1824 - val_loss: 0.1732\n",
            "Epoch 149/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1819 - val_loss: 0.1732\n",
            "Epoch 150/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1817 - val_loss: 0.1741\n",
            "Epoch 151/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1821 - val_loss: 0.1730\n",
            "Epoch 152/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1817 - val_loss: 0.1733\n",
            "Epoch 153/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1825 - val_loss: 0.1735\n",
            "Epoch 154/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1815 - val_loss: 0.1738\n",
            "Epoch 155/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1822 - val_loss: 0.1734\n",
            "Epoch 156/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1816 - val_loss: 0.1733\n",
            "Epoch 157/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1815 - val_loss: 0.1743\n",
            "Epoch 158/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1818 - val_loss: 0.1744\n",
            "Epoch 159/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1819 - val_loss: 0.1737\n",
            "Epoch 160/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1808 - val_loss: 0.1777\n",
            "Epoch 161/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1812 - val_loss: 0.1736\n",
            "Epoch 162/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1812 - val_loss: 0.1725\n",
            "Epoch 163/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1812 - val_loss: 0.1741\n",
            "Epoch 164/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1809 - val_loss: 0.1732\n",
            "Epoch 165/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1805 - val_loss: 0.1801\n",
            "Epoch 166/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1816 - val_loss: 0.1726\n",
            "Epoch 167/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1809 - val_loss: 0.1726\n",
            "Epoch 168/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1813 - val_loss: 0.1750\n",
            "Epoch 169/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1807 - val_loss: 0.1772\n",
            "Epoch 170/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1815 - val_loss: 0.1770\n",
            "Epoch 171/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1813 - val_loss: 0.1756\n",
            "Epoch 172/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1821 - val_loss: 0.1814\n",
            "Epoch 173/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1813 - val_loss: 0.1850\n",
            "Epoch 174/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1815 - val_loss: 0.1727\n",
            "Epoch 175/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1814 - val_loss: 0.1729\n",
            "Epoch 176/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1806 - val_loss: 0.1774\n",
            "Epoch 177/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1810 - val_loss: 0.1733\n",
            "Epoch 178/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1811 - val_loss: 0.1774\n",
            "Epoch 179/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1810 - val_loss: 0.1726\n",
            "Epoch 180/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1813 - val_loss: 0.1726\n",
            "Epoch 181/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1815 - val_loss: 0.1837\n",
            "Epoch 182/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1803 - val_loss: 0.1752\n",
            "Epoch 183/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1811 - val_loss: 0.1787\n",
            "Epoch 184/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1814 - val_loss: 0.1736\n",
            "Epoch 185/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1816 - val_loss: 0.1755\n",
            "Epoch 186/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1812 - val_loss: 0.1741\n",
            "Epoch 187/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1811 - val_loss: 0.1736\n",
            "Epoch 188/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1804 - val_loss: 0.1729\n",
            "Epoch 189/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1810 - val_loss: 0.1776\n",
            "Epoch 190/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1804 - val_loss: 0.1731\n",
            "Epoch 191/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1808 - val_loss: 0.1727\n",
            "Epoch 192/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1808 - val_loss: 0.1725\n",
            "Epoch 193/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1806 - val_loss: 0.1729\n",
            "Epoch 194/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1806 - val_loss: 0.1734\n",
            "Epoch 195/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1806 - val_loss: 0.1741\n",
            "Epoch 196/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1812 - val_loss: 0.1724\n",
            "Epoch 197/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1806 - val_loss: 0.1775\n",
            "Epoch 198/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1804 - val_loss: 0.1895\n",
            "Epoch 199/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1807 - val_loss: 0.1734\n",
            "Epoch 200/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1807 - val_loss: 0.1743\n",
            "Epoch 201/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1808 - val_loss: 0.1738\n",
            "Epoch 202/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1810 - val_loss: 0.1742\n",
            "Epoch 203/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1803 - val_loss: 0.1727\n",
            "Epoch 204/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1803 - val_loss: 0.1729\n",
            "Epoch 205/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1808 - val_loss: 0.1734\n",
            "Epoch 206/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1809 - val_loss: 0.1736\n",
            "Epoch 207/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1810 - val_loss: 0.1725\n",
            "Epoch 208/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1805 - val_loss: 0.1726\n",
            "Epoch 209/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1803 - val_loss: 0.1752\n",
            "Epoch 210/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1811 - val_loss: 0.1730\n",
            "Epoch 211/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1803 - val_loss: 0.1759\n",
            "Epoch 212/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1807 - val_loss: 0.1750\n",
            "Epoch 213/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1804 - val_loss: 0.1719\n",
            "Epoch 214/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1806 - val_loss: 0.1844\n",
            "Epoch 215/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1807 - val_loss: 0.1897\n",
            "Epoch 216/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1801 - val_loss: 0.1734\n",
            "Epoch 217/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1806 - val_loss: 0.1767\n",
            "Epoch 218/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1798 - val_loss: 0.1726\n",
            "Epoch 219/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1803 - val_loss: 0.1719\n",
            "Epoch 220/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1805 - val_loss: 0.1733\n",
            "Epoch 221/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1801 - val_loss: 0.1739\n",
            "Epoch 222/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1800 - val_loss: 0.1719\n",
            "Epoch 223/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1804 - val_loss: 0.1742\n",
            "Epoch 224/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1800 - val_loss: 0.1722\n",
            "Epoch 225/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1730\n",
            "Epoch 226/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1808 - val_loss: 0.1728\n",
            "Epoch 227/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1750\n",
            "Epoch 228/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1798 - val_loss: 0.1738\n",
            "Epoch 229/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1798 - val_loss: 0.1746\n",
            "Epoch 230/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1796 - val_loss: 0.1722\n",
            "Epoch 231/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1801 - val_loss: 0.1800\n",
            "Epoch 232/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1805 - val_loss: 0.1735\n",
            "Epoch 233/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1798 - val_loss: 0.1756\n",
            "Epoch 234/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1725\n",
            "Epoch 235/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1717\n",
            "Epoch 236/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1911\n",
            "Epoch 237/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1798 - val_loss: 0.1717\n",
            "Epoch 238/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1791 - val_loss: 0.1719\n",
            "Epoch 239/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1758\n",
            "Epoch 240/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1791 - val_loss: 0.1717\n",
            "Epoch 241/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1712\n",
            "Epoch 242/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1757\n",
            "Epoch 243/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1730\n",
            "Epoch 244/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1738\n",
            "Epoch 245/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1730\n",
            "Epoch 246/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1714\n",
            "Epoch 247/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1775\n",
            "Epoch 248/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1809 - val_loss: 0.1715\n",
            "Epoch 249/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1730\n",
            "Epoch 250/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1724\n",
            "Epoch 251/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1803 - val_loss: 0.1859\n",
            "Epoch 252/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1790 - val_loss: 0.1715\n",
            "Epoch 253/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1724\n",
            "Epoch 254/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1741\n",
            "Epoch 255/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1739\n",
            "Epoch 256/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1752\n",
            "Epoch 257/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1795 - val_loss: 0.1717\n",
            "Epoch 258/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1735\n",
            "Epoch 259/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1753\n",
            "Epoch 260/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1806 - val_loss: 0.1710\n",
            "Epoch 261/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1799 - val_loss: 0.1765\n",
            "Epoch 262/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1799 - val_loss: 0.1728\n",
            "Epoch 263/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1787 - val_loss: 0.1744\n",
            "Epoch 264/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1798 - val_loss: 0.1715\n",
            "Epoch 265/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1788\n",
            "Epoch 266/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1749\n",
            "Epoch 267/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1735\n",
            "Epoch 268/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1725\n",
            "Epoch 269/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1718\n",
            "Epoch 270/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1720\n",
            "Epoch 271/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1748\n",
            "Epoch 272/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1800 - val_loss: 0.1721\n",
            "Epoch 273/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1786 - val_loss: 0.1746\n",
            "Epoch 274/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1726\n",
            "Epoch 275/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1713\n",
            "Epoch 276/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1726\n",
            "Epoch 277/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1796 - val_loss: 0.1714\n",
            "Epoch 278/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1787 - val_loss: 0.1734\n",
            "Epoch 279/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1805 - val_loss: 0.1722\n",
            "Epoch 280/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1743\n",
            "Epoch 281/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1790 - val_loss: 0.1737\n",
            "Epoch 282/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1761\n",
            "Epoch 283/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1721\n",
            "Epoch 284/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1786 - val_loss: 0.1788\n",
            "Epoch 285/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1718\n",
            "Epoch 286/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1789 - val_loss: 0.1727\n",
            "Epoch 287/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1737\n",
            "Epoch 288/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1712\n",
            "Epoch 289/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1722\n",
            "Epoch 290/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1714\n",
            "Epoch 291/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1771\n",
            "Epoch 292/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1787 - val_loss: 0.1743\n",
            "Epoch 293/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1723\n",
            "Epoch 294/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1785 - val_loss: 0.1789\n",
            "Epoch 295/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1717\n",
            "Epoch 296/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1768\n",
            "Epoch 297/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1724\n",
            "Epoch 298/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1748\n",
            "Epoch 299/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1729\n",
            "Epoch 300/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1791 - val_loss: 0.1717\n",
            "Epoch 301/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1732\n",
            "Epoch 302/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1797 - val_loss: 0.1734\n",
            "Epoch 303/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1743\n",
            "Epoch 304/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1788 - val_loss: 0.1720\n",
            "Epoch 305/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1713\n",
            "Epoch 306/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1720\n",
            "Epoch 307/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1785 - val_loss: 0.1754\n",
            "Epoch 308/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1788 - val_loss: 0.1717\n",
            "Epoch 309/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1730\n",
            "Epoch 310/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1718\n",
            "Epoch 311/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1732\n",
            "Epoch 312/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1716\n",
            "Epoch 313/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1727\n",
            "Epoch 314/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1795 - val_loss: 0.1715\n",
            "Epoch 315/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1731\n",
            "Epoch 316/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1799 - val_loss: 0.1787\n",
            "Epoch 317/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1789\n",
            "Epoch 318/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1714\n",
            "Epoch 319/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1769\n",
            "Epoch 320/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1792 - val_loss: 0.1730\n",
            "Epoch 321/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1804 - val_loss: 0.1719\n",
            "Epoch 322/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1790 - val_loss: 0.1727\n",
            "Epoch 323/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1722\n",
            "Epoch 324/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1724\n",
            "Epoch 325/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1788 - val_loss: 0.1724\n",
            "Epoch 326/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1719\n",
            "Epoch 327/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1802 - val_loss: 0.1728\n",
            "Epoch 328/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1721\n",
            "Epoch 329/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1793 - val_loss: 0.1744\n",
            "Epoch 330/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1718\n",
            "Epoch 331/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1730\n",
            "Epoch 332/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1786 - val_loss: 0.1724\n",
            "Epoch 333/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1719\n",
            "Epoch 334/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1766\n",
            "Epoch 335/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1717\n",
            "Epoch 336/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1811 - val_loss: 0.1748\n",
            "Epoch 337/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1799 - val_loss: 0.1722\n",
            "Epoch 338/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1803 - val_loss: 0.1755\n",
            "Epoch 339/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1737\n",
            "Epoch 340/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1788 - val_loss: 0.1744\n",
            "Epoch 341/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1721\n",
            "Epoch 342/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1735\n",
            "Epoch 343/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1802 - val_loss: 0.1734\n",
            "Epoch 344/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1794 - val_loss: 0.1730\n",
            "Epoch 345/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1809 - val_loss: 0.1726\n",
            "Epoch 346/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1728\n",
            "Epoch 347/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1788 - val_loss: 0.1766\n",
            "Epoch 348/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1759\n",
            "Epoch 349/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1739\n",
            "Epoch 350/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1733\n",
            "Epoch 351/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1723\n",
            "Epoch 352/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1779\n",
            "Epoch 353/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1790\n",
            "Epoch 354/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1808 - val_loss: 0.1741\n",
            "Epoch 355/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1805 - val_loss: 0.1745\n",
            "Epoch 356/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1727\n",
            "Epoch 357/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1740\n",
            "Epoch 358/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1815\n",
            "Epoch 359/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1797 - val_loss: 0.1731\n",
            "Epoch 360/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1789 - val_loss: 0.1724\n",
            "Epoch 361/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1792 - val_loss: 0.1740\n",
            "Epoch 362/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1713\n",
            "Epoch 363/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1798 - val_loss: 0.1717\n",
            "Epoch 364/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1735\n",
            "Epoch 365/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1733\n",
            "Epoch 366/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1725\n",
            "Epoch 367/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1716\n",
            "Epoch 368/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1750\n",
            "Epoch 369/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1788 - val_loss: 0.1789\n",
            "Epoch 370/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1795 - val_loss: 0.1730\n",
            "Epoch 371/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1792 - val_loss: 0.1730\n",
            "Epoch 372/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1730\n",
            "Epoch 373/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1796 - val_loss: 0.1737\n",
            "Epoch 374/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1789 - val_loss: 0.1731\n",
            "Epoch 375/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1790 - val_loss: 0.1716\n",
            "Epoch 376/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1717\n",
            "Epoch 377/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1731\n",
            "Epoch 378/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1788 - val_loss: 0.1730\n",
            "Epoch 379/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1764\n",
            "Epoch 380/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1804 - val_loss: 0.1735\n",
            "Epoch 381/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1794 - val_loss: 0.1724\n",
            "Epoch 382/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1798 - val_loss: 0.1713\n",
            "Epoch 383/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1729\n",
            "Epoch 384/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1794 - val_loss: 0.1717\n",
            "Epoch 385/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1791 - val_loss: 0.1769\n",
            "Epoch 386/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1795 - val_loss: 0.1719\n",
            "Epoch 387/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1795 - val_loss: 0.1818\n",
            "Epoch 388/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1803 - val_loss: 0.1766\n",
            "Epoch 389/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1799 - val_loss: 0.1741\n",
            "Epoch 390/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1800 - val_loss: 0.1718\n",
            "Epoch 391/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1778\n",
            "Epoch 392/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1791 - val_loss: 0.1724\n",
            "Epoch 393/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1796 - val_loss: 0.1726\n",
            "Epoch 394/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1793 - val_loss: 0.1745\n",
            "Epoch 395/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1797 - val_loss: 0.1738\n",
            "Epoch 396/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1801 - val_loss: 0.1718\n",
            "Epoch 397/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1792 - val_loss: 0.1740\n",
            "Epoch 398/400\n",
            "1896/1896 [==============================] - 3s 2ms/step - loss: 0.1798 - val_loss: 0.1718\n",
            "Epoch 399/400\n",
            "1896/1896 [==============================] - 2s 1ms/step - loss: 0.1795 - val_loss: 0.1748\n",
            "Epoch 400/400\n",
            "1896/1896 [==============================] - 3s 1ms/step - loss: 0.1793 - val_loss: 0.1725\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Definición del modelo de tensorflow:\n",
        "modelo = tf.keras.Sequential()\n",
        "#Definición de la capa 1\n",
        "modelo.add(tf.keras.layers.Dense(8, activation='relu'))\n",
        "#Definición de la capa 2\n",
        "modelo.add(tf.keras.layers.Dense(5, activation='relu'))\n",
        "#Definición de la capa 3, con una neurona por cada salida deseada.\n",
        "modelo.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "modelo.compile(optimizer='rmsprop', loss='mse')\n",
        "history = modelo.fit(inputs_train, outputs_train, epochs=400, batch_size=4, validation_data=(inputs_validate, outputs_validate))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#El modelo es utilizado para realizar las predicciones de los input de prueba\n",
        "predictions = modelo.predict(inputs_test)\n",
        "\n",
        "print(\"Predicciones = \\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "plt.title('Datos predichos del set de entrenamiento vs valores actuales')\n",
        "#plt.plot(inputs_test, outputs_test, label='Actual')\n",
        "plt.plot(inputs_test, predictions, label='Predicho')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "CuPrQxc_bEbu",
        "outputId": "c1c21ec5-00d7-4a55-94ee-871922251f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 0s 3ms/step\n",
            "Predicciones = \n",
            " [[0.059 0.941]\n",
            " [0.433 0.567]\n",
            " [0.94  0.06 ]\n",
            " ...\n",
            " [0.059 0.941]\n",
            " [0.989 0.011]\n",
            " [0.372 0.628]]\n",
            "actual =\n",
            " [[0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " ...\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-65e7f47b4c98>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datos predichos del set de entrenamiento vs valores actuales'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(inputs_test, outputs_test, label='Actual')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicho'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mncx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mncx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mncy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mncx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mncy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x has {ncx} columns but y has {ncy} columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mncx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mncy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x has 3 columns but y has 2 columns"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversión del modelo a tflite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
        "lite_model = converter.convert()\n",
        "\n",
        "open(\"fire_model.tflite\", \"wb\").write(lite_model)\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "basic_model_size = os.path.getsize(\"fire_model.tflite\")\n",
        "print(\"Model is %d bytes\" %basic_model_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsdHN7lDcIZY",
        "outputId": "18392fca-ffa8-464b-e5bf-cf81d4e356d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is 2480 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafccd68-ac81-42fe-c53b-ef3b77c75bef"
      },
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat fire_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file, model.h, is 15,328 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4U1yxlUqdCoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}